{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing \n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "\n",
    "    # information on the model: https://arxiv.org/abs/2209.07562\n",
    "    def __init__(self, model: str = \"Twitter/twhin-bert-base\"):\n",
    "        self.model = dict(\n",
    "            tokenizer=transformers.AutoTokenizer.from_pretrained(model),\n",
    "            transformer=transformers.AutoModel.from_pretrained(model),\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: typing.List[str]):\n",
    "        return self._pool(\n",
    "            self.model[\"transformer\"](\n",
    "                **self.model[\"tokenizer\"](\n",
    "                    batch,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "            )\n",
    "            .last_hidden_state\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _pool(batch: torch.Tensor, method: typing.Literal[\"mean\", \"cls\"] = \"mean\"):\n",
    "        return dict(\n",
    "            mean=lambda x: x.mean(dim=1),\n",
    "            cls=lambda x: x[:, 0, :]\n",
    "        )[method](batch)\n",
    "\n",
    "    @property\n",
    "    def num_dim(self) -> int:\n",
    "        return self.model[\"transformer\"].config.to_dict()[\"hidden_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at Twitter/twhin-bert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Encoded data dimensionality: 768'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder: Encoder = Encoder()\n",
    "f\"Encoded data dimensionality: {encoder.num_dim}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "history: typing.List[str] = [\n",
    "    \"Democrats scream every day about limiting Americans rights and pretend to care about going after tax cheats, so why are they totally silent when it comes to Hunter Biden?.\",\n",
    "    \"If our economy is doing as well as JoeBiden seems to think under his policies, then why is inflation up 16.6%?!  Either he is lying or in la la land. Clearly, his policies have failed hardworking American families.\"\n",
    "]\n",
    "feed: typing.List[str] = [\n",
    "    \"JoeBiden's weak appeasement policies are hurting our country and letting down our allies. Why is he giving evil regimes and thugs a pass?\",\n",
    "    \"Donald Trump tried to use Justice Department officials not as independent fact finders, but as partisan surrogates to legitimize his Big Lie.\",\n",
    "    \"Uncle Nearest Premium Whiskey Honored As Wine Enthusiast's 2020 Spirit Brand Of The Year\",\n",
    "    \"at subway: And just a little lettuce. the guy starts backing a truck full of lettuce toward my sandwich\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_matrix: torch.Tensor = encoder(history)\n",
    "history_matrix.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_matrix: torch.Tensor = encoder(feed)\n",
    "feed_matrix.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = torch.nn.CosineSimilarity(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity(history, feed_0) = 0.0828'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'similarity(history, feed_1) = 0.0712'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'similarity(history, feed_2) = 0.0572'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'similarity(history, feed_3) = 0.0583'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n, post_vector in enumerate(feed_matrix):\n",
    "    display(\n",
    "        f\"similarity(history, feed_{n}) = {similarity_score(history_matrix.mean(), post_vector).item():2.4f}\"\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
