{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/apis/TWON-Agents/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "import os\n",
    "\n",
    "import pandas\n",
    "import datasets\n",
    "import trl\n",
    "import peft\n",
    "\n",
    "import cltrier_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/apis/TWON-Agents/.venv/lib/python3.10/site-packages/transformers/training_args.py:2046: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case simon-muenker/Llama-3.2-3B-Instruct-OSN-replies).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_SLUG: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "UPLOAD_SLUG: str = f\"{MODEL_SLUG.split('/')[-1]}-OSN-replies\"\n",
    "\n",
    "RAW_DATASET: str = \"../data/interim/twitter.german.dataset.preds.csv\"\n",
    "\n",
    "SFT_ARGS = trl.SFTConfig(\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=4,\n",
    "    packing=True, \n",
    "    save_strategy=\"no\",\n",
    "    output_dir=f\"../models/{UPLOAD_SLUG}\",\n",
    "    logging_steps=50,\n",
    "    push_to_hub=True,\n",
    "    push_to_hub_model_id=UPLOAD_SLUG\n",
    ")\n",
    "\n",
    "PEFT_ARGS = peft.LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'messages': [{'role': 'system',\n",
       "    'content': 'You are a social media user with a political neutral leaning. Respond to the following Tweet:'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Nicht der #Verbrenner schadet dem #Klima, sondern der fossile Sprit, mit dem er f√§hrt. Wir haben diese Woche den Weg f√ºr klimaneutrale #eFuels freigemacht. Damit k√∂nnten die mehr als 45 Mio. Diesel- und Benzin-Fahrzeuge auf unseren Stra√üen in Zukunft klimaneutral unterwegs sein.'},\n",
       "   {'role': 'assistant', 'content': 'Sie haben wirklich keine Ahnung.'}]},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are a social media user with a political neutral leaning. Respond to the following Tweet:'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Wo waren die ganzen pl√∂tzlichen #Kernkraftbef√ºrworter in #Altparteien, Verb√§nden &amp; Medien in den letzten Jahren? Warum stimmte die #umfaller: #fdp bis zuletzt im Bundestag gegen Laufzeitverl√§ngerungen? Fakt ist: nur die #AfD lag von Anfang an &amp; jahrelang richtig &amp; blieb auf Kurs!'},\n",
       "   {'role': 'assistant', 'content': 'N√∂. Die liegt permanent falsch.'}]},\n",
       " {'messages': [{'role': 'system',\n",
       "    'content': 'You are a social media user with a political right leaning. Respond to the following Tweet:'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Die FDP Ostwestfalen-Lippe spricht sich f√ºr die Verl√§ngerung der Laufzeiten der Kernkraftwerke aus.'},\n",
       "   {'role': 'assistant', 'content': 'Dann bauen wir doch da ein KKW hin.'}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset: typing.List[typing.Dict] = [\n",
    "    cltrier_lib.inference.schemas.Chat(messages=[\n",
    "        cltrier_lib.inference.schemas.Message(role=\"system\", content=f\"You are a social media user with a political {row['leaning']} leaning. Respond to the following Tweet:\"),\n",
    "        cltrier_lib.inference.schemas.Message(role=\"user\", content=row[\"text_post\"]),\n",
    "        cltrier_lib.inference.schemas.Message(role=\"assistant\", content=row[\"text_reply\"])\n",
    "    ]).model_dump()\n",
    "    for _, row in pandas.read_csv(RAW_DATASET, index_col=0).iterrows()\n",
    "]\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/apis/TWON-Agents/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:185: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.82it/s]\n",
      "/home/ubuntu/apis/TWON-Agents/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1856' max='1856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1856/1856 1:01:17, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.781700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.789700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.625400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.585300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.563700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.529300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.524700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors:   0%|          | 0.00/9.19M [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "training_args.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.62k/5.62k [00:00<00:00, 20.4kB/s]5.5MB/s]\n",
      "adapter_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.19M/9.19M [00:00<00:00, 9.34MB/s]\n",
      "Upload 2 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.62it/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "trainer = trl.SFTTrainer(\n",
    "    MODEL_SLUG,\n",
    "    args=SFT_ARGS,\n",
    "    train_dataset=datasets.Dataset.from_pandas(pandas.DataFrame(data=dataset)),\n",
    "    peft_config=PEFT_ARGS,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(SFT_ARGS.output_dir)\n",
    "\n",
    "if SFT_ARGS.push_to_hub:\n",
    "    trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
