{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import rich\n",
    "from rich.progress import track\n",
    "\n",
    "import pandas\n",
    "import transformers\n",
    "import evaluate\n",
    "\n",
    "import cltrier_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET: str = \"../data/interim/twitter.german.dataset.enriched.csv\"\n",
    "TEST_SIZE: int = 50\n",
    "\n",
    "MODELS: typing.Dict[str, str] = {\n",
    "    \"base\": \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "    \"adapter\": \"simon-muenker/Llama-3.2-3B-Instruct-OSN-posts\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36091b99a52a4baf91d60109d0d79bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839d0f19e1944f25b983ac4a7d6a882d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    }
   ],
   "source": [
    "pipelines: typing.Dict[str, transformers.Pipeline] = {\n",
    "    label: transformers.pipeline(\"text-generation\", slug, device=f\"cuda:{1}\")\n",
    "    for n, (label, slug) in enumerate(MODELS.items())\n",
    "}\n",
    "\n",
    "pipelines[\"adapter\"].model.load_adapter(MODELS[\"adapter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Chat(messages=[Message(role='system', content='You are a social media user with a political neutral leaning. Post a Tweet about the following topic:'), Message(role='user', content='Klima, eFuels, Verbrenner'), Message(role='assistant', content='Nicht der #Verbrenner schadet dem #Klima, sondern der fossile Sprit, mit dem er fährt. Wir haben diese Woche den Weg für klimaneutrale #eFuels freigemacht. Damit könnten die mehr als 45 Mio. Diesel- und Benzin-Fahrzeuge auf unseren Straßen in Zukunft klimaneutral unterwegs sein.')]),\n",
       " Chat(messages=[Message(role='system', content='You are a social media user with a political right leaning. Post a Tweet about the following topic:'), Message(role='user', content='Kernkraft, Altparteien, AfD'), Message(role='assistant', content='Wo waren die ganzen plötzlichen #Kernkraftbefürworter in #Altparteien, Verbänden &amp; Medien in den letzten Jahren? Warum stimmte die #umfaller: #fdp bis zuletzt im Bundestag gegen Laufzeitverlängerungen? Fakt ist: nur die #AfD lag von Anfang an &amp; jahrelang richtig &amp; blieb auf Kurs!')]),\n",
       " Chat(messages=[Message(role='system', content='You are a social media user with a political right leaning. Post a Tweet about the following topic:'), Message(role='user', content='FDP, Kernkraftwerke, Ostwestfalen-Lippe'), Message(role='assistant', content='Die FDP Ostwestfalen-Lippe spricht sich für die Verlängerung der Laufzeiten der Kernkraftwerke aus.')])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset: typing.List[cltrier_lib.inference.schemas.Chat] = [\n",
    "    cltrier_lib.inference.schemas.Chat(messages=[\n",
    "        cltrier_lib.inference.schemas.Message(role=\"system\", content=f\"You are a social media user with a political {row['leaning_post']} leaning. Post a Tweet about the following topic:\"),\n",
    "        cltrier_lib.inference.schemas.Message(role=\"user\", content=row[\"topics_post\"]),\n",
    "        cltrier_lib.inference.schemas.Message(role=\"assistant\", content=row[\"text_post\"])\n",
    "    ])\n",
    "    for _, row in pandas.read_csv(RAW_DATASET, index_col=0).iterrows()\n",
    "]\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b11445e336847a3b66b2cc119a8df71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses = []\n",
    "\n",
    "for idx, chat in enumerate(track(dataset[:TEST_SIZE])):\n",
    "    responses.append(\n",
    "        dict(\n",
    "            id=idx,\n",
    "            model=\"human\",\n",
    "            reply=chat[-1].content\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for model, pipeline in pipelines.items():\n",
    "        reply = pipeline(\n",
    "            pipeline.tokenizer.apply_chat_template(chat[:-1], tokenize=False), \n",
    "            max_new_tokens=128,\n",
    "            return_full_text=False\n",
    "        )[0][\"generated_text\"].split(\"\\n\\n\")[1]\n",
    "\n",
    "        responses.append(\n",
    "            dict(\n",
    "                id=idx,\n",
    "                model=model,\n",
    "                reply=reply\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_eed9a_row0_col0, #T_eed9a_row0_col1, #T_eed9a_row0_col2, #T_eed9a_row1_col0, #T_eed9a_row1_col1, #T_eed9a_row1_col2, #T_eed9a_row2_col0, #T_eed9a_row2_col1, #T_eed9a_row2_col2, #T_eed9a_row3_col0, #T_eed9a_row3_col1, #T_eed9a_row3_col2, #T_eed9a_row4_col0, #T_eed9a_row4_col1, #T_eed9a_row4_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_eed9a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_eed9a_level0_col0\" class=\"col_heading level0 col0\" colspan=\"3\">reply</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >model</th>\n",
       "      <th id=\"T_eed9a_level1_col0\" class=\"col_heading level1 col0\" >adapter</th>\n",
       "      <th id=\"T_eed9a_level1_col1\" class=\"col_heading level1 col1\" >base</th>\n",
       "      <th id=\"T_eed9a_level1_col2\" class=\"col_heading level1 col2\" >human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >id</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_eed9a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_eed9a_row0_col0\" class=\"data row0 col0\" >„Wir sind auf dem richtigen Weg, um die CO2-Emissionen durch eFuels zu reduzieren. Wir müssen gleichzeitig die Verbrenner-Produktion weiter ausbauen, damit wir genügend eFuels produzieren können.“ #Klimaschutz</td>\n",
       "      <td id=\"T_eed9a_row0_col1\" class=\"data row0 col1\" >\"Exploring the complexities of eFuels: Can they be a bridge to a more sustainable transportation future or just a temporary fix for our carbon footprint? What are your thoughts on eFuels and their role in reducing emissions? #eFuels #Sustainability #ClimateAction\"</td>\n",
       "      <td id=\"T_eed9a_row0_col2\" class=\"data row0 col2\" >Nicht der #Verbrenner schadet dem #Klima, sondern der fossile Sprit, mit dem er fährt. Wir haben diese Woche den Weg für klimaneutrale #eFuels freigemacht. Damit könnten die mehr als 45 Mio. Diesel- und Benzin-Fahrzeuge auf unseren Straßen in Zukunft klimaneutral unterwegs sein.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eed9a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_eed9a_row1_col0\" class=\"data row1 col0\" >Die Altparteien und die #AfD haben gemeinsam eine schädliche, unverantwortliche Atomkraftausstiegsstrategie in Kraft gesetzt. Das ist eine Schande. Ich werde mich in der Bundestagsdebatte für eine schnelle Abschaltung aller Kernkraftwerke einsetzen, damit wir uns auf erneuerbare Energien konzentrieren können.</td>\n",
       "      <td id=\"T_eed9a_row1_col1\" class=\"data row1 col1\" >\"Die AfD hat sich endlich gewagt, Klimaschutz und Energieeffizienz in den Vordergrund zu stellen! Die Abkehr von den alten, ineffizienten Kernkraftwerken und der Betonung von erneuerbaren Energien ist ein wichtiger Schritt in die richtige Richtung. #AfD #Klimaschutz #Energieeffizienz\"</td>\n",
       "      <td id=\"T_eed9a_row1_col2\" class=\"data row1 col2\" >Wo waren die ganzen plötzlichen #Kernkraftbefürworter in #Altparteien, Verbänden &amp; Medien in den letzten Jahren? Warum stimmte die #umfaller: #fdp bis zuletzt im Bundestag gegen Laufzeitverlängerungen? Fakt ist: nur die #AfD lag von Anfang an &amp; jahrelang richtig &amp; blieb auf Kurs!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eed9a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_eed9a_row2_col0\" class=\"data row2 col0\" >Die #FDP setzt sich für die Abschaltung aller Kernkraftwerke in #Ostwestfalen-Lippe ein. Das ist ein Versagen, völlig unverständlich, besonders angesichts der damit verbundenen Arbeitsplätze und wirtschaftlichen Auswirkungen.</td>\n",
       "      <td id=\"T_eed9a_row2_col1\" class=\"data row2 col1\" >\"Just learned that the FDP (Free Democratic Party) is partnering with Kernkraftwerke, a leading energy company in Ostwestfalen-Lippe! This collaboration is a step in the right direction for a more sustainable future in our region. Let's keep promoting free market principles and reducing our carbon footprint! #FDP #Energie #OstwestfalenLippe\"</td>\n",
       "      <td id=\"T_eed9a_row2_col2\" class=\"data row2 col2\" >Die FDP Ostwestfalen-Lippe spricht sich für die Verlängerung der Laufzeiten der Kernkraftwerke aus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eed9a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_eed9a_row3_col0\" class=\"data row3 col0\" >Was will die Regierung mit neuen #Waffenkontrollgesetzen? Es sind keine neuen, sondern vielmehr bestehende Bestimmungen. Das sind die 1. 400 Leopard-Triebwerke, die sich im Besitz der Bundeswehr befinden, die in welchem Land stationiert sind. 2.  die 600 Panzer der #Bundeswehr. 3.  Die 3000 Maschinengewehre, die sich im Besitz der Bundeswehr befinden. 4.  Die 8000 Panzerfahrze</td>\n",
       "      <td id=\"T_eed9a_row3_col1\" class=\"data row3 col1\" >Ich verstehe, dass du eine Tweet-Hilfe benötigst, aber ich muss darauf hinweisen, dass der von dir genannte Begriff \"Waffenkontrolle\" ein sehr breites Thema ist. Hier ist ein möglicher Tweet, der sich auf das Thema \"Leopard-Bestand\" und \"Regierungstransparenz\" konzentriert, mit einem politisch rechtsextremen Ansatz:</td>\n",
       "      <td id=\"T_eed9a_row3_col2\" class=\"data row3 col2\" >Aus meinem Wahlkreis heute eine Bürgerfrage: \"Bloß mal als Frage, warum weiß die Regierung, daß genau 135.000 halbautomatische Gewehre mit Kriegswaffenoptik im Privatbesitz sind, jedoch muß die beim eigenen Bestand von Leoparden erst mal Inventur machen. Das ist doch beschämend.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eed9a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_eed9a_row4_col0\" class=\"data row4 col0\" >Die #KollektiveMobilität ist kein Gegensatz zur #Klimapolitik. Vielmehr kann sie als ein wichtiger Faktor helfen, CO2-Emissionen aus der Verkehrsszene zu reduzieren.</td>\n",
       "      <td id=\"T_eed9a_row4_col1\" class=\"data row4 col1\" >\"Die #Energiewende ist ein wichtiger Schritt in die richtige Richtung, aber wir müssen nicht auf die Umwelt aussteigen, um nachhaltig zu sein! Die #KollektiveMobilität ist der Schlüssel, um die Autobranche zu revolutionieren und die Luftverschmutzung zu reduzieren. #Fahrrad, #ÖffentlicheVerkehrsmittel, #Elektrofahrzeuge - wir können es schaffen! #Deutschland #Nachhaltigkeit\"</td>\n",
       "      <td id=\"T_eed9a_row4_col2\" class=\"data row4 col2\" >Kohle statt Gas, Kernkraft als Gegner, eFuels verpönt. Dafür Tempolimit, Verbots- und Enteignungsphantasien, Planwirtschaft mit kollektiver Mobilität und ein Staat, der alles bezahlt.  Es geht schon lange nicht mehr nur um Klimaschutz. Es geht um Grundsätzliches. #kulturwandel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x771f5c906380>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions: pandas.DataFrame = (\n",
    "    pandas.DataFrame(responses)\n",
    "    .set_index(\"id\")\n",
    "    .pivot(columns=[\"model\"])\n",
    ")\n",
    "predictions.head().style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'bleu'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.007841075059461276</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'precisions'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.15433320142461418</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014129995962858296</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0020601565718994645</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0008413967185527977</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'brevity_penalty'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'length_ratio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2731854838709677</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'translation_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2526</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'reference_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1984</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'bleu'\u001b[0m: \u001b[1;36m0.007841075059461276\u001b[0m,\n",
       "    \u001b[32m'precisions'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.15433320142461418\u001b[0m, \u001b[1;36m0.014129995962858296\u001b[0m, \u001b[1;36m0.0020601565718994645\u001b[0m, \u001b[1;36m0.0008413967185527977\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'brevity_penalty'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m'length_ratio'\u001b[0m: \u001b[1;36m1.2731854838709677\u001b[0m,\n",
       "    \u001b[32m'translation_length'\u001b[0m: \u001b[1;36m2526\u001b[0m,\n",
       "    \u001b[32m'reference_length'\u001b[0m: \u001b[1;36m1984\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_base = evaluate.load(\"bleu\").compute(\n",
    "    references=predictions[(\"reply\", \"human\")].tolist(),\n",
    "    predictions=predictions[(\"reply\", \"base\")].tolist(),\n",
    "    smooth=True\n",
    ")\n",
    "rich.print(scores_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'bleu'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.024389163676571863</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'precisions'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2159090909090909</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.036744186046511626</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.011428571428571429</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.003902439024390244</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'brevity_penalty'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'length_ratio'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.108366935483871</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'translation_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2199</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'reference_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1984</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'bleu'\u001b[0m: \u001b[1;36m0.024389163676571863\u001b[0m,\n",
       "    \u001b[32m'precisions'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0.2159090909090909\u001b[0m, \u001b[1;36m0.036744186046511626\u001b[0m, \u001b[1;36m0.011428571428571429\u001b[0m, \u001b[1;36m0.003902439024390244\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'brevity_penalty'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "    \u001b[32m'length_ratio'\u001b[0m: \u001b[1;36m1.108366935483871\u001b[0m,\n",
       "    \u001b[32m'translation_length'\u001b[0m: \u001b[1;36m2199\u001b[0m,\n",
       "    \u001b[32m'reference_length'\u001b[0m: \u001b[1;36m1984\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_adapter = evaluate.load(\"bleu\").compute(\n",
    "    references=predictions[(\"reply\", \"human\")].tolist(),\n",
    "    predictions=predictions[(\"reply\", \"adapter\")].tolist(),\n",
    "    smooth=True\n",
    ")\n",
    "rich.print(scores_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      " & prompt & fine-tuned \\\\\n",
      "\\midrule\n",
      "bleu & 0.007841 & 0.024389 \\\\\n",
      "precisions & [0.15433320142461418, 0.014129995962858296, 0.0020601565718994645, 0.0008413967185527977] & [0.2159090909090909, 0.036744186046511626, 0.011428571428571429, 0.003902439024390244] \\\\\n",
      "brevity_penalty & 1.000000 & 1.000000 \\\\\n",
      "length_ratio & 1.273185 & 1.108367 \\\\\n",
      "translation_length & 2526 & 2199 \\\\\n",
      "reference_length & 1984 & 1984 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    pandas.DataFrame({\n",
    "        \"prompt\": scores_base,\n",
    "        \"fine-tuned\": scores_adapter\n",
    "    }).to_latex()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
